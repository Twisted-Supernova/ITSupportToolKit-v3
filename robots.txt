# robots.txt for IT Support Toolkit
# Last updated: 2025-01-02
# Purpose: Allow legitimate search engines, block AI scrapers and bad bots

# ============================================================================
# ALLOWED: Legitimate Search Engines
# ============================================================================

# Google
User-agent: Googlebot
Allow: /

# Bing
User-agent: Bingbot
Allow: /

# DuckDuckGo
User-agent: DuckDuckBot
Allow: /

# Yahoo
User-agent: Slurp
Allow: /

# Yandex (Russian search engine)
User-agent: YandexBot
Allow: /

# Baidu (Chinese search engine)
User-agent: Baiduspider
Allow: /

# ============================================================================
# BLOCKED: AI Scrapers and Training Bots
# ============================================================================

# OpenAI (ChatGPT)
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

# Anthropic (Claude)
User-agent: Claude-Web
Disallow: /

User-agent: anthropic-ai
Disallow: /

# Google AI training (separate from search)
User-agent: Google-Extended
Disallow: /

# Common Crawl (used for AI training datasets)
User-agent: CCBot
Disallow: /

# Facebook/Meta AI
User-agent: FacebookBot
Disallow: /

User-agent: Meta-ExternalAgent
Disallow: /

# Cohere AI
User-agent: cohere-ai
Disallow: /

# Perplexity AI
User-agent: PerplexityBot
Disallow: /

# Bytespider (TikTok/ByteDance)
User-agent: Bytespider
Disallow: /

# Amazon AI
User-agent: Amazonbot
Disallow: /

# Apple AI
User-agent: Applebot-Extended
Disallow: /

# ============================================================================
# BLOCKED: Aggressive Scrapers and Bad Bots
# ============================================================================

# AhrefsBot (SEO crawler - very aggressive)
User-agent: AhrefsBot
Disallow: /

# SemrushBot (SEO crawler - very aggressive)
User-agent: SemrushBot
Disallow: /

# MJ12bot (Majestic SEO)
User-agent: MJ12bot
Disallow: /

# DotBot (Moz crawler)
User-agent: DotBot
Disallow: /

# Scrapy (web scraping framework)
User-agent: Scrapy
Disallow: /

# Generic bad bots
User-agent: SiteSnagger
Disallow: /

User-agent: WebCopier
Disallow: /

User-agent: WebStripper
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: Offline Explorer
Disallow: /

User-agent: Teleport
Disallow: /

User-agent: WebZIP
Disallow: /

# ============================================================================
# DEFAULT RULE: Allow all other bots (cautious approach)
# ============================================================================

User-agent: *
Allow: /

# ============================================================================
# SITEMAP LOCATION (Optional - create later if needed)
# ============================================================================

# Sitemap: https://itsupporttoolkit.netlify.app/sitemap.xml
